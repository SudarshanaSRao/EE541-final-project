{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt8vtPRtEhMw"
      },
      "source": [
        "### Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY45UPJuEhM3",
        "outputId": "3a8dde43-d02c-454f-ad75-6ce939992239"
      },
      "outputs": [],
      "source": [
        "%pip install opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPSn_wX3EhM5"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SUAgSf3EhM5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import opendatasets as od\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Subset\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from utils import ProgressBar, plot_metrics\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q2zFt90EhM6"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYvBiGTzEhM6",
        "outputId": "789dd3cd-cb16-47be-8480-00dbcc2db868"
      },
      "outputs": [],
      "source": [
        "# Download Kaggle dataset (Kaggle username and key is required)\n",
        "# {\"username\":\"christopherconroy\",\"key\":\"1915e76943ae798bc236fb7c2de6d28d\"}\n",
        "od.download('https://www.kaggle.com/datasets/grassknoted/asl-alphabet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtjxxNkEEhM7"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "TEST_DATA_DIR = 'asl-alphabet/asl_alphabet_test/asl_alphabet_test'\n",
        "TRAIN_DATA_DIR = 'asl-alphabet/asl_alphabet_train/asl_alphabet_train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDgjOcVmEhM8",
        "outputId": "0843fcd3-cfb8-48ff-c374-8c5b942a86d8"
      },
      "outputs": [],
      "source": [
        "# Initialize dataset\n",
        "dataset = torchvision.datasets.ImageFolder(root = TRAIN_DATA_DIR, transform = ToTensor())\n",
        "num_inputs = np.array(dataset[0][0].numpy().shape).prod()\n",
        "num_outputs = len(dataset.classes)\n",
        "\n",
        "# Check for CUDA GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using {device} device')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ISSRVcEhM-"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset, num_samples, train_split, batch_size, seed = 0):\n",
        "    '''\n",
        "    Create training and test data loaders from the given dataset.\n",
        "\n",
        "    Params:\n",
        "        dataset = PyTorch Dataset instance for full dataset\n",
        "        num_samples = Number of samples to use from the full dataset\n",
        "        train_split = Fraction of train data in train/test split\n",
        "        batch_size Minibatch size for training\n",
        "        seed = Random seed for dataset shuffle and split. Set to None for no seed.\n",
        "\n",
        "    Returns:\n",
        "        train_loader = Dataloader for the training samples\n",
        "        test_loader = Dataloader for the test samples\n",
        "    '''\n",
        "\n",
        "    # Perform stratified split of dataset indicies\n",
        "    train_size = int((num_samples * train_split) // batch_size) * batch_size\n",
        "    test_size = num_samples - train_size\n",
        "    dataset_inds = list(range(len(dataset)))\n",
        "    train_inds, test_inds = train_test_split(dataset_inds, train_size = train_size, test_size = test_size, random_state = seed, stratify = dataset.targets)\n",
        "\n",
        "    # Create training and test subsets\n",
        "    train_set = Subset(dataset, train_inds)\n",
        "    test_set = Subset(dataset, test_inds)\n",
        "\n",
        "    # Initialize data loader\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mNkxR2AEhM_"
      },
      "source": [
        "### Training Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKxvcsnlEhNA"
      },
      "outputs": [],
      "source": [
        "def train(data_loader, model, loss_func, optimizer):\n",
        "    # Initialize parameters\n",
        "    size = len(data_loader.dataset)\n",
        "    num_batches = len(data_loader)\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Set mode to training\n",
        "    model.train()\n",
        "\n",
        "    # Initialize progress bar\n",
        "    progress = ProgressBar('Train Progress', len(data_loader))\n",
        "\n",
        "    # Iterate through batches\n",
        "    for images, labels in data_loader: \n",
        "        # Transfer images and labels to GPU\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass \n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimization\n",
        "        optimizer.step()\n",
        "\n",
        "        # Transfer outputs and labels to CPU\n",
        "        outputs, labels = outputs.cpu(), labels.cpu()\n",
        "        \n",
        "        # Compute batch metrics\n",
        "        total_loss += loss.item()\n",
        "        pred = torch.max(outputs, 1)[1]\n",
        "        correct += (pred == labels).sum().numpy()\n",
        "\n",
        "        # Update progress\n",
        "        progress.step()\n",
        "\n",
        "    # Compute metrics for dataset\n",
        "    total_loss /= num_batches\n",
        "    accuracy = (correct / size) * 100\n",
        "\n",
        "    return total_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8pfhoEzEhNB"
      },
      "outputs": [],
      "source": [
        "def test(data_loader, model, loss_func):\n",
        "    # Initialize parameters\n",
        "    size = len(data_loader.dataset)\n",
        "    num_batches = len(data_loader)\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Set mode to evaluation\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize progress bar\n",
        "    progress = ProgressBar('Valid Progress', len(data_loader))\n",
        "\n",
        "    # Iterate through batches\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            # Transfer images and labels to GPU\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = loss_func(outputs, labels)\n",
        "\n",
        "            # Transfer outputs and labels to CPU\n",
        "            outputs, labels = outputs.cpu(), labels.cpu()\n",
        "\n",
        "            # Compute batch metrics\n",
        "            total_loss += loss.item()\n",
        "            pred = torch.max(outputs, 1)[1]\n",
        "            correct += (pred == labels).sum().numpy()\n",
        "\n",
        "            # Update progress\n",
        "            progress.step()\n",
        "            \n",
        "    # Compute metrics for dataset\n",
        "    total_loss /= num_batches\n",
        "    accuracy = (correct / size) * 100\n",
        "\n",
        "    return total_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIBp-4gdEhNC"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, valid_loader, learning_rate, num_epochs, weight_decay = 0):\n",
        "    # Initialize training parameters\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "\n",
        "    # Initialize metrics\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    train_accuracy_list = []\n",
        "    test_accuracy_list = []\n",
        "\n",
        "    # Train model\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train and evaluate model\n",
        "        train_loss, train_accuracy = train(train_loader, model, loss_func, optimizer)\n",
        "        valid_loss, valid_accuracy = test(valid_loader, model, loss_func)\n",
        "\n",
        "        # Store epoch metrics\n",
        "        train_loss_list.append(train_loss)\n",
        "        train_accuracy_list.append(train_accuracy)\n",
        "        test_loss_list.append(valid_loss)\n",
        "        test_accuracy_list.append(valid_accuracy)\n",
        "\n",
        "        # Output progress\n",
        "        print('Epoch {} | Loss = {:.4f} | Train Accuracy = {:.2f}% | Test Accuracy = {:.2f}%'.format(epoch + 1, train_loss, train_accuracy, valid_accuracy))\n",
        "\n",
        "    return (train_loss_list, train_accuracy_list), (test_loss_list, test_accuracy_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR6C5dyTEhND"
      },
      "source": [
        "### Model #1 - MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "PTJusiyWEhNE",
        "outputId": "78173e26-2ae9-4a50-e5a6-18dbf7aed260"
      },
      "outputs": [],
      "source": [
        "googlenet = models.googlenet(pretrained = True)\n",
        "googlenet = googlenet.to(device)\n",
        "train_loader, valid_loader = split_dataset(dataset, 1000, 0.8, 10)\n",
        "train_metrics, valid_metrics = train_model(googlenet, train_loader, valid_loader, 0.001, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsUhQgL-EhNE"
      },
      "outputs": [],
      "source": [
        "plot_metrics(train_metrics, valid_metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('ee541')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5c4c54ecf42830e23b91be3e35800f11de85c5f48dbf4f514b6a1ba40b2b37fe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
