{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opendatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import opendatasets as od\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle dataset (Kaggle username and key is required)\n",
    "# {\"username\":\"christopherconroy\",\"key\":\"1915e76943ae798bc236fb7c2de6d28d\"}\n",
    "od.download('https://www.kaggle.com/datasets/grassknoted/asl-alphabet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TEST_DATA_DIR = 'asl-alphabet/asl_alphabet_test/asl_alphabet_test'\n",
    "TRAIN_DATA_DIR = 'asl-alphabet/asl_alphabet_train/asl_alphabet_train'\n",
    "NUM_SAMPLES = 1000  # Number of dataset samples used (<= len(dataset))\n",
    "TRAIN_SPLIT = 0.8  # Fraction of train data in train/valid split\n",
    "SEED = 0  # Random seed for dataset shuffle and split\n",
    "BATCH_SIZE = 100  # Minibatch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=TRAIN_DATA_DIR,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "num_inputs = np.array(dataset[0][0].numpy().shape).prod()\n",
    "num_outputs = len(dataset.classes)\n",
    "\n",
    "# Perform stratified split of dataset indicies\n",
    "train_size = int((NUM_SAMPLES * TRAIN_SPLIT) // BATCH_SIZE) * BATCH_SIZE\n",
    "test_size = NUM_SAMPLES - train_size\n",
    "dataset_inds = list(range(len(dataset)))\n",
    "train_inds, valid_inds = train_test_split(dataset_inds, train_size=train_size, \n",
    "        test_size=test_size, random_state=SEED, stratify=dataset.targets)\n",
    "\n",
    "# Create training and validation subsets\n",
    "train_set = Subset(dataset, train_inds)\n",
    "valid_set = Subset(dataset, valid_inds)\n",
    "\n",
    "# Initialize data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Check for CUDA GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, loss_func, optimizer):\n",
    "    # Initialize parameters\n",
    "    size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # Set mode to training\n",
    "    model.train()\n",
    "\n",
    "    # Initialize progress bar\n",
    "    progress = ProgressBar('Train Progress', len(data_loader))\n",
    "\n",
    "    # Iterate through batches\n",
    "    for images, labels in data_loader: \n",
    "        # Transfer images and labels to GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, num_inputs)\n",
    "        \n",
    "        # Forward pass \n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Transfer outputs and labels to CPU\n",
    "        outputs, labels = outputs.cpu(), labels.cpu()\n",
    "        \n",
    "        # Compute batch metrics\n",
    "        total_loss += loss.item()\n",
    "        pred = torch.max(outputs, 1)[1]\n",
    "        correct += (pred == labels).sum().numpy()\n",
    "\n",
    "        # Update progress\n",
    "        progress.step()\n",
    "\n",
    "    # Compute metrics for dataset\n",
    "    total_loss /= num_batches\n",
    "    accuracy = (correct / size) * 100\n",
    "\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader, model, loss_func):\n",
    "    # Initialize parameters\n",
    "    size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # Set mode to evaluation\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize progress bar\n",
    "    progress = ProgressBar('Valid Progress', len(data_loader))\n",
    "\n",
    "    # Iterate through batches\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            # Transfer images and labels to GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.view(-1, num_inputs)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Transfer outputs and labels to CPU\n",
    "            outputs, labels = outputs.cpu(), labels.cpu()\n",
    "\n",
    "            # Compute batch metrics\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.max(outputs, 1)[1]\n",
    "            correct += (pred == labels).sum().numpy()\n",
    "\n",
    "            # Update progress\n",
    "            progress.step()\n",
    "            \n",
    "    # Compute metrics for dataset\n",
    "    total_loss /= num_batches\n",
    "    accuracy = (correct / size) * 100\n",
    "\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, num_epochs, weight_decay=0):\n",
    "    # Initialize training parameters\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize metrics\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train and evaluate model\n",
    "        train_loss, train_accuracy = train(train_loader, model, loss_func, optimizer)\n",
    "        valid_loss, valid_accuracy = test(valid_loader, model, loss_func)\n",
    "\n",
    "        # Store epoch metrics\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        test_loss_list.append(valid_loss)\n",
    "        test_accuracy_list.append(valid_accuracy)\n",
    "\n",
    "        # Output progress\n",
    "        print('Epoch {} | Train Accuracy = {:.2f}% | Test Accuracy = {:.2f}%'\n",
    "            .format(epoch + 1, train_accuracy, valid_accuracy))\n",
    "\n",
    "    return (train_loss_list, train_accuracy_list), (test_loss_list, test_accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.hidden1 = nn.Linear(num_inputs, 128)\n",
    "        self.output = nn.Linear(128, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model1 = Net1()\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics1, test_metrics1 = train_model(model1, 0.01, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ee541')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c4c54ecf42830e23b91be3e35800f11de85c5f48dbf4f514b6a1ba40b2b37fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
