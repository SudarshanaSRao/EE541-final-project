{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from utils import split_dataset, train_model, plot_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TEST_DATA_DIR = 'asl-alphabet/asl_alphabet_test/asl_alphabet_test'\n",
    "TRAIN_DATA_DIR = 'asl-alphabet/asl_alphabet_train/asl_alphabet_train'\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=TRAIN_DATA_DIR,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "num_inputs = np.array(dataset[0][0].numpy().shape).prod()\n",
    "num_outputs = len(dataset.classes)\n",
    "\n",
    "# Check for CUDA GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - MLP\n",
    "\n",
    "This simple model is used to verify that the supporting data loading, splitting, training and validation functions are working correctly by training the model on a small subset of the data. The model metrics should show distinct decreasing loss and attain a high training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.hidden1 = nn.Linear(num_inputs, 1000)\n",
    "        self.hidden2 = nn.Linear(1000, 200)\n",
    "        self.output = nn.Linear(200, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model1 = Net1()\n",
    "model1 = model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader1, valid_loader1 = split_dataset(dataset, 58, 0.5, 29, SEED)\n",
    "train_metrics1, valid_metrics1 = train_model(model1, train_loader1, valid_loader1, 5e-4, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(train_metrics1, valid_metrics1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2 - Base CNN\n",
    "\n",
    "This model is a simple CNN used to verify that such an architecture is able to train on the data and provide baseline performance. The architecture is based on the baseline model used in this [paper](https://ieeexplore.ieee.org/document/8392660) but does not make use of dropout layers or any other form of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layer 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        # self.dropout1 = nn.Dropout2d(0.2)\n",
    "\n",
    "        # Convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        # self.dropout2 = nn.Dropout2d(0.2)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        self.fc1 = nn.Linear(32 * 50**2, 128)\n",
    "        # self.dropout3 = nn.Dropout2d(0.2)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # self.dropout4 = nn.Dropout2d(0.2)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(64, 29)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model2 = Net2()\n",
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model2, input_size=(3, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2, valid_loader2 = split_dataset(dataset, 1000, 0.8, 100, SEED)\n",
    "train_metrics2, valid_metrics2 = train_model(model2, train_loader2, valid_loader2, 5e-4, 10, device, conv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(train_metrics2, valid_metrics2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ee541')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c4c54ecf42830e23b91be3e35800f11de85c5f48dbf4f514b6a1ba40b2b37fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
